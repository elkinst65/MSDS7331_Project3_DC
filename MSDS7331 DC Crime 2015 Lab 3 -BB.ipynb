{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7331 - Lab Three: Association Rule Mining\n",
    "\n",
    "\n",
    "### Investigators\n",
    "- [Matt Baldree](mailto:mbaldree@smu.edu?subject=lab2)\n",
    "- [Tom Elkins](telkins@smu.edu?subject=lab2)\n",
    "- [Austin Kelly](ajkelly@smu.edu?subject=lab2)\n",
    "- [Ben Brock](bbrock@smu.edu?subject=lab2)\n",
    "\n",
    "\n",
    "<div style='margin-left:10%;margin-right:10%;margin-top:15px;background-color:#d3d3d3;padding:5px;'>\n",
    "    <h3>Lab Instructions</h3>\n",
    "    <p>You are to build upon the predictive analysis that you already completed in the previous mini-project, adding additional modeling from new classification algorithms as well as more explanations that are inline with the CRISP-DM framework. You should use appropriate cross validation for all of your analysis (explain your chosen method of performance validation <i>in detail</i>). Try to use as much testing data as possible <i>in a realistic manner</i> (you should define what you think is realistic and why).</p>\n",
    "    <p>This report is worth 20% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a single document. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.</p>\n",
    "    <p>Report Sections:</p>\n",
    "    <ol>\n",
    "        <li>[Data Preparation](#data_preparation) <b>(15 points)</b></li>\n",
    "        <li>[Modeling and Evaluation](#modeling_and_evaluation) <b>(70 points)</b></li>\n",
    "        <li>[Deployment](#deployment) <b>(5 points)</b></li>\n",
    "        <li>[Exceptional Work](#exceptional_work) <b>(10 points)</b></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-Essential Packages to Add for Windows 10-64 Bit Computer in the Python 2.7 Environment\n",
    "\n",
    "For the last project (Project 3: \"CRISP-DM Capstone\") of the Data Mining course, our team decided to challenge ourselves and step outside our comfort zone and take the challenge to use Association Rule Mining option to determine Association Rules for input DC Crime data set.  We took heed to our professor's recommendation, to use the \"R\" programming language, the R-Essential \"mlbench\" package, and the R-Essential \"arules\" package as tools to determine the Association Rules for the DC Crime data set.    \n",
    "\n",
    "\n",
    "![Ward Map](images/R-Reproducibility-Problem.jpg \"R Reproducibility\") \n",
    "<p style='text-align: center;'>\n",
    "R-Reproducibility-Problem\n",
    "</p>\n",
    "\n",
    "\n",
    "### How to Compile R-Essential Packages Windows 10-64 Bit Python 2.7 Environment\n",
    "\n",
    "I googled the Internet to understand how to build and install any generic CRAN R-Essential package for Windows 10-64 bit machine.  It was duly noted that the \"mlbench\" and \"arules\" package are NOT part of the basic R-Essential packages for Anaconda.  Therefore, this means our team had to build and install \"mlbench\", \"arules\", and all other supporting R-Essential packages needed. \n",
    "\n",
    "![Ward Map](images/How-To-Install-R-Essentials.jpg \"How-To-Install-R-Essentials\") \n",
    "<p style='text-align: center;'>\n",
    "How-To-Install-R-Essentials\n",
    "</p>\n",
    "\n",
    "\n",
    "Another interesting fact is that Anaconda provides enterprise support (phone or personal support) for Anaconda Pro, Anaconda Workgroup, and Anaconda Enterprise. The Anaconda free download only provides community support.  All of our team members are using the free version of Anaconda.  Luckily, I worked with Jim Morrison of Continuum Analytics to help me with the R-Essential package installations for the Windows-10 64 bit machine.   Jim Morrison provided free support.   \n",
    "\n",
    "![Ward Map](images/AnacondaSupport.jpg \"Anaconda Support\") \n",
    "<p style='text-align: center;'>\n",
    "Anaconda Support\n",
    "</p>\n",
    "\n",
    "\n",
    "Below are the 21 R-Essential packages that our team member installed to get the \"mlbench\" and \"arules\" to compile successfully in our Python 27 environment. The (19) nineteen other R-Essential packages are the additional packages that were needed to be built/compiled and installed. As a side note, the team member created a new Anaconda Python environment to install the R-Essential packages.  All of the 21 R-Essential packages listed in the table below are readily available on the Anaconda Cloud under the name \"benbrock26\".  \n",
    "\n",
    "|CRAN Package Name|Description\n",
    "|:------|:----------------|\n",
    "|mlbench| Machine Learning Benchmark Problems| \n",
    "|arules|Mining Association Rules and Frequent Itemsets|\n",
    "|r-ggplot2| Create Elegant Data Visualisations Using the Grammar of Graphics |\n",
    "|r-scales| Scale Functions for Visualization | \n",
    "|r-purr| Functional Programming Tools | \n",
    "| r-viridislite  | Default Color Maps from 'matplotlib' (Lite Version) | \n",
    "| r-seriation  | Infrastructure for Ordering Objects Using Seriation | \n",
    "| r-tsp  | Traveling Salesperson Problem (TSP) |\n",
    "| r-fpc  | Flexible Procedures for Clustering |\n",
    "| r-dendextend  | Extending R's Dendrogram Functionality |\n",
    "| r-diptest | Hartigan's Dip Test Statistic for Unimodality - Corrected |\n",
    "| r-flexmix | Flexible Mixture Modeling |\n",
    "| r-kernlab | Kernel-Based Machine Learning Lab |\n",
    "| r-prabclus | Functions for Clustering of Presence-Absence, Abundance and Multilocus Genetic Data |\n",
    "| r-mclust | Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation |\n",
    "| r-robustbase | Basic Robust Statistics |\n",
    "| r-deoptimr | Differential Evolution Optimization in Pure R |\n",
    "| r-trimcluster | Cluster analysis with trimming |\n",
    "| r-gclus | Clustering Graphics |\n",
    "| r-qap | Heuristics for the Quadratic Assignment Problem (QAP) |\n",
    "| r-vcd | Visualizing Categorical Data |\n",
    "\n",
    "\n",
    "**Note: The above slides are from Christine Doig who is a Senior Data Scientist working at Continuum Analytics (reference slides from opendatascientwithrandanaconav22.pdf).**\n",
    "\n",
    "### 100% Python 2.7 Solution for Association Rule Mining\n",
    "\n",
    "After successfully compiling all of the R-Essential packages below without any errors, the Python environment became unstable.  Therefore, our team decided to look at a 100% Python 2.7 solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100% Python 2.7 Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our team started early on this project.  We knew it was going to require a major commitment to get this project done properly.  Being it was nearing the end of the Fall 2016 semester, time management was going to be key to be successful to understand and solve this new data for mining associations rules paradigm. Our team knew we had to gain a strong understanding of the problem so that we could implement the problem.  The other Data Science course that we were taking in the program this Fall 2016 semester, MSDS7349 Data and Network Security, was more busier than any of us ever thought it would be.   \n",
    "\n",
    "Therefore, we selected to use a divide a conquer and strategy.  Two of our team members decided to work on the Association Rule Mining to determine the association rules, while the other two members worked on the clustering problem. Then the two members of each team decided to work individually to see if we came up with the same results. \n",
    "\n",
    "I was very fortunate to find a 100% Apriori Python implementation by Timothy Asp and Caleb Carlton.  I found the code on GitHub (https://github.com/timothyasp/apriori-python).   My big assumption was that the code worked as advertised.  Once we determined that the code worked as advertised by exercising all of the unit tests, then we would apply this to our DC Crime data set.  Our goal was to use this code as a working library just as any python data mining, machine learning, or generic python library.  Our team member manually executed all of the unit test cases that were provided by Timothy Asp and Caleb Carlton.  To gain a better understanding of the code, we decided to capture the software design in a Object Oriented Analysis and Design (OOAD) UML class diagram. \n",
    "\n",
    "![Ward Map](images/AssociationRulesMiningUnitTest.jpg \"Association Rules Mining Unit Test Class Diagram\") \n",
    "<p style='text-align: center;'>\n",
    "Association Rules Mining Unit Test Class Diagram\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC Crime Goods.csv Excel File \n",
    "\n",
    "Here, we discuss the design of the DC Crime goods.csv file.  It is important that every item (or good) of the DC Crime data set is listed and accounted for.  There are a total of 505 items in the DC Crime data.  The inventory list describes list the ID number along with the description of the good.  Here, we had to sit back abstractly think of each available item or good that is available in the DC Crime data set.\n",
    "\n",
    "|ID| description|category | number| Type| \n",
    "|:------|:----------------| :--- \n",
    "|0| Theft/Other|Offense | 1 | OFFENSE_Code| \n",
    "|1| Theft from Auto|Offense | 2 | OFFENSE_Code|\n",
    "|2| Buglary |Offense | 3 | OFFENSE_Code|\n",
    "|3| Assault with Dangerous Weapon | Offense| 4 |OFFENSE_Code |\n",
    "|4| Robbery|Offense | 5 | OFFENSE_Code| \n",
    "|5| Motor Vehicle|Offense | 6 | OFFENSE_Code|\n",
    "|6| Homicide |Offense | 7 | OFFENSE_Code|\n",
    "|7| Sex Abuse | Offense| 8 |OFFENSE_Code |\n",
    "|8| Arson | Offense| 9 |OFFENSE_Code |\n",
    "|9| DAY |SHIFT | 1 | SHIFT_Code|\n",
    "|10| EVENING | SHIFT | 2 |SHIFT_Code |\n",
    "|11| MIDNIGHT | SHIFT | 3 |SHIFT_Code |\n",
    "|12| OTHERS |METHOD | 1 | METHOD_Code|\n",
    "|13| GUN | METHOD | 2 |METHOD_Code |\n",
    "|14| Knife | METHOD | 3 |METHOD_Code |\n",
    "|15| Property | Crime | 1 |CRIME_TYPE |\n",
    "|16| Violent | Crime | 2 |CRIME_TYPE |\n",
    "|0| DistrictID_1|DISTRICT | 1 | DistrictID| \n",
    "|1| DistrictID_2|DISTRICT | 2 | DistrictID|\n",
    "|2| DistrictID_3 |DISTRICT | 3 | DistrictID|\n",
    "|3| DistrictID_4 | DISTRICT| 4 |DistrictID |\n",
    "|4| DistrictID_5|DISTRICT | 5 | DistrictID| \n",
    "|5| DistrictID_6|DISTRICT | 6 | DistrictID|\n",
    "|6| DistrictID_7 |DISTRICT | 7 | DistrictID|\n",
    "|..| .....|..... | .. | ......|\n",
    "|..| .....|..... | .. | ......|\n",
    "|491|\tCCN_ID_0|\tCCN_ID|\t0| CCN |\n",
    "|492|\tXBLOCK_ID_0|\tXBLOCK_ID|\t1| XBLOCK|\n",
    "|493|\tYBLOCK_ID_0|\tYBLOCK_ID|\t2|\tYBLOCK|\n",
    "|494|\tAGE_ID_0|\tAGE_ID| 3| AGE|\n",
    "|495|\tTIME_TO_REPORT_ID_0|\tTIME_TO_REPORT_ID|\t4|TIME_TO_REPORT|\n",
    "|496|\tLATITUDE_ID_0|\tLATITUDE_ID| 5\t| Latitude|\n",
    "|497|\tLONGITUDE_ID_0|\tLONGITUDE_ID| 6\t| Longitude|\n",
    "|498|\tMAX_TEMP_ID_0|\tMAX_TEMP_ID| 7\t| Max_Temp|\n",
    "|499|\tMIN_TEMP_ID_0|\tMIN_TEMP_ID| 8\t| Min_Temp|\n",
    "|500|\tMAX_HUMIDITY_ID_0|\tMAX_HUMIDITY_ID|\t9  | Max_Humidity|\n",
    "|501|\tMIN_HUMIDITY_ID_0|\tMIN_HUMIDITY_ID|\t10 | Min_Humidity|\n",
    "|502|\tMAX_PRESSURE_ID_0|\tMAX_PRESSURE_ID|\t11 | Max_Pressure|\n",
    "|503|\tMIN_PRESSURE_ID_0|\tMIN_PRESSURE_ID|\t12 | Min_Pressure|\n",
    "|504|\tPRECIPITATION_ID_0|\tPRECIPITATION_ID|\t13 | Precipitation|\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DC Crime 2015 Complete Transaction csv File\n",
    "\n",
    "|TID| WARD|ANC | NEIGHBORHOOD_CLUSTER| CENSUS_TRACT| VOTING_PRECINCT | CCN |XBLOCK  |YBLOCK |PSA_ID | DistrictID| SHIFT_Code| OFFENSE_Code | METHOD_Code | CRIME_TYPE | \n",
    "|:------|:----------------| :--- | :--| |:------|:----------------|\n",
    "|1| 80 | 89 | 130|230| 369 | 14849 | 505 | 7612 | 44 | 19 | 9 | 0 | 12 | 16 |\n",
    "|2| 80 | 91 | 129|209| 387 | 14850 | 506 | 7613 | 55 | 20 | 9 | 1 | 12 | 16 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Apriori Association Rule Mining Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected input format: python apriori.py <dataset.csv> <minSup> <minConf>\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://github.com/timothyasp/apriori-python/blob/master/apriori.py\n",
    "'''\n",
    "import sys\n",
    "import os.path\n",
    "import csv\n",
    "import math \n",
    "import types\n",
    "from collections import defaultdict, Iterable\n",
    "import itertools\n",
    "\n",
    "class Apriori:\n",
    "    def __init__(self, data, minSup, minConf):\n",
    "        self.dataset = data\n",
    "        self.transList = defaultdict(list)\n",
    "        self.freqList = defaultdict(int)\n",
    "        self.itemset = set()\n",
    "        self.highSupportList = list()\n",
    "        self.numItems = 0\n",
    "        self.prepData()             # initialize the above collections\n",
    "\n",
    "        self.F = defaultdict(list)\n",
    "\n",
    "        self.minSup = minSup\n",
    "        self.minConf = minConf\n",
    "\n",
    "    def genAssociations(self):\n",
    "        candidate = {}\n",
    "        count = {}\n",
    "\n",
    "        self.F[1] = self.firstPass(self.freqList, 1)\n",
    "        k=2\n",
    "        while len(self.F[k-1]) != 0:\n",
    "            candidate[k] = self.candidateGen(self.F[k-1], k)\n",
    "            for t in self.transList.iteritems():\n",
    "                for c in candidate[k]:\n",
    "                    if set(c).issubset(t[1]):\n",
    "                        self.freqList[c] += 1\n",
    "\n",
    "            self.F[k] = self.prune(candidate[k], k)\n",
    "            if k > 2:\n",
    "                self.removeSkyline(k, k-1)\n",
    "            k += 1\n",
    "\n",
    "        return self.F\n",
    "\n",
    "    def removeSkyline(self, k, kPrev):\n",
    "        for item in self.F[k]:\n",
    "            subsets = self.genSubsets(item)\n",
    "            for subset in subsets:\n",
    "                if subset in (self.F[kPrev]):\n",
    "                    self.F[kPrev].remove(subset)\n",
    "                    \n",
    "\n",
    "        subsets = self.genSubsets\n",
    "\n",
    "    def prune(self, items, k):\n",
    "        f = []\n",
    "        for item in items:\n",
    "            count = self.freqList[item]\n",
    "            support = self.support(count)\n",
    "            if support >= .95:\n",
    "                self.highSupportList.append(item)\n",
    "            elif support >= self.minSup:\n",
    "                f.append(item)\n",
    "\n",
    "        return f\n",
    "\n",
    "    def candidateGen(self, items, k):\n",
    "        candidate = []\n",
    "\n",
    "        if k == 2:\n",
    "            candidate = [tuple(sorted([x, y])) for x in items for y in items if len((x, y)) == k and x != y]\n",
    "        else:\n",
    "            candidate = [tuple(set(x).union(y)) for x in items for y in items if len(set(x).union(y)) == k and x != y]\n",
    "        \n",
    "        for c in candidate:\n",
    "            subsets = self.genSubsets(c)\n",
    "            if any([ x not in items for x in subsets ]):\n",
    "                candidate.remove(c)\n",
    "\n",
    "        return set(candidate)\n",
    "\n",
    "    def genSubsets(self, item):\n",
    "        subsets = []\n",
    "        for i in range(1,len(item)):\n",
    "            subsets.extend(itertools.combinations(item, i))\n",
    "        return subsets\n",
    "\n",
    "    def genRules(self, F):\n",
    "        H = []\n",
    "\n",
    "        for k, itemset in F.iteritems():\n",
    "            if k >= 2:\n",
    "                for item in itemset:\n",
    "                    subsets = self.genSubsets(item)\n",
    "                    for subset in subsets:\n",
    "                        if len(subset) == 1:\n",
    "                            subCount = self.freqList[subset[0]]\n",
    "                        else:\n",
    "                            subCount = self.freqList[subset]\n",
    "                        itemCount = self.freqList[item]\n",
    "                        if subCount != 0:\n",
    "                            confidence = self.confidence(subCount, itemCount)\n",
    "                            if confidence >= self.minConf:\n",
    "                                support = self.support(self.freqList[item])\n",
    "                                rhs = self.difference(item, subset)\n",
    "                                if len(rhs) == 1:\n",
    "                                    H.append((subset, rhs, support, confidence))\n",
    "\n",
    "        return H\n",
    "\n",
    "    def difference(self, item, subset):\n",
    "        return tuple(x for x in item if x not in subset)\n",
    "\n",
    "    def confidence(self, subCount, itemCount):\n",
    "        return float(itemCount)/subCount\n",
    "\n",
    "    def support(self, count):\n",
    "        return float(count)/self.numItems\n",
    "\n",
    "    def firstPass(self, items, k):\n",
    "        f = []\n",
    "        for item, count in items.iteritems():\n",
    "            support = self.support(count)\n",
    "            if support == 1:\n",
    "                self.highSupportList.append(item)\n",
    "            elif support >= self.minSup:\n",
    "                f.append(item)\n",
    "\n",
    "        return f\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare the transaction data into a dictionary\n",
    "    key: Receipt.id\n",
    "    val: set(Goods.Id) \n",
    "    Also generates the frequent itemlist for itemsets of size 1\n",
    "    key: Goods.Id\n",
    "    val: frequency of Goods.Id in self.transList\n",
    "    \"\"\"\n",
    "    def prepData(self):\n",
    "        key = 0\n",
    "        for basket in self.dataset:\n",
    "            self.numItems += 1\n",
    "            key = basket[0]\n",
    "            for i, item in enumerate(basket):\n",
    "                if i != 0:\n",
    "                    self.transList[key].append(item.strip())\n",
    "                    self.itemset.add(item.strip())\n",
    "                    self.freqList[(item.strip())] += 1\n",
    "                    \n",
    "                    \n",
    "def main():\n",
    "    goods = defaultdict(list)\n",
    "    num_args = len(sys.argv)\n",
    "    minSup = minConf = 0\n",
    "    noRules = False\n",
    "\n",
    "    # Make sure the right number of input files are specified\n",
    "    if  num_args < 4 or num_args > 5:\n",
    "        print 'Expected input format: python apriori.py <dataset.csv> <minSup> <minConf>'\n",
    "        return\n",
    "    elif num_args == 5 and sys.argv[1] == \"--no-rules\":\n",
    "        dataset = csv.reader(open(sys.argv[2], \"r\"))\n",
    "        goodsData = csv.reader(open('goods.csv', \"r\"))\n",
    "        minSup  = float(sys.argv[3])\n",
    "        minConf = float(sys.argv[4])\n",
    "        noRules = True\n",
    "        print \"Dataset: \", sys.argv[2], \" MinSup: \", minSup, \" MinConf: \", minConf\n",
    "    else: \n",
    "        dataset = csv.reader(open(sys.argv[1], \"r\"))\n",
    "        goodsData = csv.reader(open('goods.csv', \"r\"))\n",
    "        minSup  = float(sys.argv[2])\n",
    "        minConf = float(sys.argv[3])\n",
    "        print \"Dataset: \", sys.argv[1], \" MinSup: \", minSup, \" MinConf: \", minConf\n",
    "\n",
    "    print \"==================================================================\"\n",
    "\n",
    "    for item in goodsData:\n",
    "        goods[item[0]] = item[1:]\n",
    "\n",
    "    a = Apriori(dataset, minSup, minConf)\n",
    "\n",
    "    frequentItemsets = a.genAssociations()\n",
    "\n",
    "    count = 0\n",
    "    for k, item in frequentItemsets.iteritems():\n",
    "        for i in item:\n",
    "            if k >= 2:\n",
    "                count += 1\n",
    "                print count,\":  \",readable(i, goods),\"\\tsupport=\",a.support(a.freqList[i])\n",
    "\n",
    "    print \"Skyline Itemsets: \", count\n",
    "    if not noRules:\n",
    "        rules = a.genRules(frequentItemsets)\n",
    "        for i, rule in enumerate(rules):\n",
    "            print \"Rule\",i+1,\":\\t \",readable(rule[0], goods),\"\\t-->\",readable(rule[1], goods),\"\\t [sup=\",rule[2],\" conf=\",rule[3],\"]\"\n",
    "\n",
    "    print \"\\n\"\n",
    "\n",
    "def readable(item, goods):\n",
    "    itemStr = ''\n",
    "    for k, i in enumerate(item):\n",
    "        itemStr += goods[i][0] + \" \" + goods[i][1] +\" (\" + i + \")\"\n",
    "        if len(item) != 0 and k != len(item)-1:\n",
    "            itemStr += \",\\t\"\n",
    "\n",
    "    return itemStr.replace(\"'\", \"\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
